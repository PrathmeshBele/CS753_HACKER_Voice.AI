{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GE2E_Voice.AI.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkAmJhp0Efsj",
        "outputId": "6dd3b424-26d4-4085-877e-f96d888f64fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/753_hacker\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/753_hacker"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create numpy files from the .wav files in the dataset"
      ],
      "metadata": {
        "id": "jUls_c9LFLg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python data_preprocess.py"
      ],
      "metadata": {
        "id": "Zr4v4azxEtYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import librosa\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.autograd as grad\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "aoLQzzQoFKy3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hparam import hparam as hp\n",
        "from data_load import SpeakerDatasetTIMIT, SpeakerDatasetTIMITPreprocessed\n",
        "\n",
        "from speech_embedder_net import SpeechEmbedder\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "pyFVPasVFdK9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "get_centroid and get_cossim "
      ],
      "metadata": {
        "id": "01c__dtcJaTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_utterance_centroids(embeddings):\n",
        "    \"\"\"\n",
        "    Returns the centroids for each utterance of a speaker, where\n",
        "    the utterance centroid is the speaker centroid without considering\n",
        "    this utterance\n",
        "\n",
        "    Shape of embeddings should be:\n",
        "        (speaker_ct, utterance_per_speaker_ct, embedding_size)\n",
        "    \"\"\"\n",
        "    sum_centroids = embeddings.sum(dim=1)\n",
        "    # we want to subtract out each utterance, prior to calculating the\n",
        "    # the utterance centroid\n",
        "    sum_centroids = sum_centroids.reshape(\n",
        "        sum_centroids.shape[0], 1, sum_centroids.shape[-1]\n",
        "    )\n",
        "    # we want the mean but not including the utterance itself, so -1\n",
        "    num_utterances = embeddings.shape[1] - 1\n",
        "    centroids = (sum_centroids - embeddings) / num_utterances\n",
        "    return centroids"
      ],
      "metadata": {
        "id": "otUVZ0WkKVYo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_centroids(embeddings):\n",
        "    centroids = embeddings.mean(dim=1)\n",
        "    return centroids\n",
        "\n",
        "\n",
        "\n",
        "def get_cossim(embeddings, centroids):\n",
        "    # number of utterances per speaker\n",
        "    num_utterances = embeddings.shape[1]\n",
        "    utterance_centroids = get_utterance_centroids(embeddings)\n",
        "\n",
        "    # flatten the embeddings and utterance centroids to just utterance,\n",
        "    # so we can do cosine similarity\n",
        "    utterance_centroids_flat = utterance_centroids.view(\n",
        "        utterance_centroids.shape[0] * utterance_centroids.shape[1],\n",
        "        -1\n",
        "    )\n",
        "    embeddings_flat = embeddings.view(\n",
        "        embeddings.shape[0] * num_utterances,\n",
        "        -1\n",
        "    )\n",
        "    # the cosine distance between utterance and the associated centroids\n",
        "    # for that utterance\n",
        "    # this is each speaker's utterances against his own centroid, but each\n",
        "    # comparison centroid has the current utterance removed\n",
        "    cos_same = F.cosine_similarity(embeddings_flat, utterance_centroids_flat)\n",
        "\n",
        "    # now we get the cosine distance between each utterance and the other speakers'\n",
        "    # centroids\n",
        "    # to do so requires comparing each utterance to each centroid. To keep the\n",
        "    # operation fast, we vectorize by using matrices L (embeddings) and\n",
        "    # R (centroids) where L has each utterance repeated sequentially for all\n",
        "    # comparisons and R has the entire centroids frame repeated for each utterance\n",
        "    centroids_expand = centroids.repeat((num_utterances * embeddings.shape[0], 1))\n",
        "    embeddings_expand = embeddings_flat.unsqueeze(1).repeat(1, embeddings.shape[0], 1)\n",
        "    embeddings_expand = embeddings_expand.view(\n",
        "        embeddings_expand.shape[0] * embeddings_expand.shape[1],\n",
        "        embeddings_expand.shape[-1]\n",
        "    )\n",
        "    cos_diff = F.cosine_similarity(embeddings_expand, centroids_expand)\n",
        "    cos_diff = cos_diff.view(\n",
        "        embeddings.size(0),\n",
        "        num_utterances,\n",
        "        centroids.size(0)\n",
        "    )\n",
        "    # assign the cosine distance for same speakers to the proper idx\n",
        "    same_idx = list(range(embeddings.size(0)))\n",
        "    cos_diff[same_idx, :, same_idx] = cos_same.view(embeddings.shape[0], num_utterances)\n",
        "    cos_diff = cos_diff + 1e-6\n",
        "    return cos_diff\n",
        "\n",
        "def calc_loss(sim_matrix):\n",
        "    same_idx = list(range(sim_matrix.size(0)))\n",
        "    pos = sim_matrix[same_idx, :, same_idx]\n",
        "    neg = (torch.exp(sim_matrix).sum(dim=2) + 1e-6).log_()\n",
        "    per_embedding_loss = -1 * (pos - neg)\n",
        "    loss = per_embedding_loss.sum()\n",
        "    return loss, per_embedding_loss"
      ],
      "metadata": {
        "id": "UgcmXoogJZ9o"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "num_workers=1\n",
        "lr = 0.01\n",
        "epochs=10\n",
        "utt = 4"
      ],
      "metadata": {
        "id": "LLkt0hsHGU--"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GE2E LOSS"
      ],
      "metadata": {
        "id": "kRp3eubdG1Ed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GE2ELoss(nn.Module):\n",
        "    \n",
        "    def __init__(self, device):\n",
        "        super(GE2ELoss, self).__init__()\n",
        "        self.w = nn.Parameter(torch.tensor(10.0).to(device), requires_grad=True)\n",
        "        self.b = nn.Parameter(torch.tensor(-5.0).to(device), requires_grad=True)\n",
        "        self.device = device\n",
        "        \n",
        "    def forward(self, embeddings):\n",
        "        torch.clamp(self.w, 1e-6)\n",
        "        centroids = get_centroids(embeddings)\n",
        "        cossim = get_cossim(embeddings, centroids)\n",
        "        sim_matrix = self.w*cossim.to(self.device) + self.b\n",
        "        loss, _ = calc_loss(sim_matrix)\n",
        "        return loss"
      ],
      "metadata": {
        "id": "5VuvnnTPG0zP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    device = torch.device(\"cuda\")\n",
        "    train_dataset = SpeakerDatasetTIMITPreprocessed()\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True) \n",
        "    \n",
        "    embedder_net = SpeechEmbedder().to(device)\n",
        "    ge2e_loss = GE2ELoss(device)\n",
        "    #Both net and loss have trainable parameters\n",
        "    optimizer = torch.optim.SGD([\n",
        "                    {'params': embedder_net.parameters()},\n",
        "                    {'params': ge2e_loss.parameters()}\n",
        "                ], lr=lr)\n",
        "    \n",
        "    os.makedirs('speech_id_checkpoint', exist_ok=True)\n",
        "    \n",
        "    embedder_net.train()\n",
        "    iteration = 0\n",
        "    for e in range(epochs):\n",
        "        print('epoch - '+str(e)+'started')\n",
        "        total_loss = 0\n",
        "        for batch_id, mel_db_batch in enumerate(train_loader): \n",
        "            mel_db_batch = mel_db_batch.to(device)\n",
        "            \n",
        "            mel_db_batch = torch.reshape(mel_db_batch, (batch_size*utt, mel_db_batch.size(2), mel_db_batch.size(3)))\n",
        "            perm = random.sample(range(0, batch_size*utt), batch_size*utt)\n",
        "            unperm = list(perm)\n",
        "            for i,j in enumerate(perm):\n",
        "                unperm[j] = i\n",
        "            mel_db_batch = mel_db_batch[perm]\n",
        "            #gradient accumulates\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            embeddings = embedder_net(mel_db_batch)\n",
        "            embeddings = embeddings[unperm]\n",
        "            embeddings = torch.reshape(embeddings, (batch_size,utt, embeddings.size(1)))\n",
        "            \n",
        "            #get loss, call backward, step optimizer\n",
        "            loss = ge2e_loss(embeddings) #wants (Speaker, Utterances, embedding)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(embedder_net.parameters(), 3.0)\n",
        "            torch.nn.utils.clip_grad_norm_(ge2e_loss.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss = total_loss + loss\n",
        "            iteration += 1\n",
        "            if (batch_id + 1) % hp.train.log_interval == 0:\n",
        "                mesg = \"{0}\\tEpoch:{1}[{2}/{3}],Iteration:{4}\\tLoss:{5:.4f}\\tTLoss:{6:.4f}\\t\\n\".format(time.ctime(), e+1,\n",
        "                        batch_id+1, len(train_dataset)//batch_size, iteration,loss, total_loss / (batch_id + 1))\n",
        "                print(mesg)\n",
        "                # if hp.train.log_file is not None:\n",
        "                #     with open(hp.train.log_file,'a') as f:\n",
        "                #         f.write(mesg)\n",
        "                    \n",
        "        if (e + 1) % 10 == 0:\n",
        "            embedder_net.eval().cpu()\n",
        "            ckpt_model_filename = \"ckpt_epoch_\" + str(e+1) + \"_batch_id_\" + str(batch_id+1) + \".pth\"\n",
        "            ckpt_model_path = os.path.join('speech_id_checkpoint', ckpt_model_filename)\n",
        "            torch.save(embedder_net.state_dict(), ckpt_model_path)\n",
        "            embedder_net.to(device).train()\n",
        "\n",
        "    #save model\n",
        "    embedder_net.eval().cpu()\n",
        "    save_model_filename = \"final_epoch_\" + str(e + 1) + \"_batch_id_\" + str(batch_id + 1) + \".model\"\n",
        "    save_model_path = os.path.join('speech_id_checkpoint', save_model_filename)\n",
        "    torch.save(embedder_net.state_dict(), save_model_path)\n",
        "    \n",
        "    print(\"\\nDone, trained model saved at\", save_model_path)\n"
      ],
      "metadata": {
        "id": "l2BzwqTIFioU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model_path):\n",
        "    \n",
        "    test_dataset = SpeakerDatasetTIMITPreprocessed()\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True)\n",
        "    \n",
        "    embedder_net = SpeechEmbedder()\n",
        "    embedder_net.load_state_dict(torch.load(model_path))\n",
        "    embedder_net.eval()\n",
        "    \n",
        "    avg_EER = 0\n",
        "    for e in range(1):\n",
        "        batch_avg_EER = 0\n",
        "        for batch_id, mel_db_batch in enumerate(test_loader):\n",
        "            assert utt % 2 == 0\n",
        "            print(len(torch.split(mel_db_batch, int(mel_db_batch.size(1)/2), dim=1)))\n",
        "            enrollment_batch, verification_batch = torch.split(mel_db_batch, int(mel_db_batch.size(1)/2), dim=1)\n",
        "            \n",
        "            enrollment_batch = torch.reshape(enrollment_batch, (batch_size*utt//2, enrollment_batch.size(2), enrollment_batch.size(3)))\n",
        "            verification_batch = torch.reshape(verification_batch, (batch_size*utt//2, verification_batch.size(2), verification_batch.size(3)))\n",
        "            \n",
        "            perm = random.sample(range(0,verification_batch.size(0)), verification_batch.size(0))\n",
        "            unperm = list(perm)\n",
        "            for i,j in enumerate(perm):\n",
        "                unperm[j] = i\n",
        "                \n",
        "            verification_batch = verification_batch[perm]\n",
        "            enrollment_embeddings = embedder_net(enrollment_batch)\n",
        "            verification_embeddings = embedder_net(verification_batch)\n",
        "            verification_embeddings = verification_embeddings[unperm]\n",
        "            \n",
        "            enrollment_embeddings = torch.reshape(enrollment_embeddings, (batch_size,utt//2, enrollment_embeddings.size(1)))\n",
        "            verification_embeddings = torch.reshape(verification_embeddings, (batch_size,utt//2, verification_embeddings.size(1)))\n",
        "            \n",
        "            enrollment_centroids = get_centroids(enrollment_embeddings)\n",
        "            \n",
        "            sim_matrix = get_cossim(verification_embeddings, enrollment_centroids)\n",
        "            \n",
        "            # calculating EER\n",
        "            diff = 1; EER=0; EER_thresh = 0; EER_FAR=0; EER_FRR=0\n",
        "            \n",
        "            for thres in [0.01*i+0.5 for i in range(50)]:\n",
        "                sim_matrix_thresh = sim_matrix>thres\n",
        "                \n",
        "                FAR = (sum([sim_matrix_thresh[i].float().sum()-sim_matrix_thresh[i,:,i].float().sum() for i in range(int(batch_size))])\n",
        "                /(batch_size-1.0)/(float(utt/2))/batch_size)\n",
        "    \n",
        "                FRR = (sum([utt/2-sim_matrix_thresh[i,:,i].float().sum() for i in range(int(hp.test.N))])\n",
        "                /(float(utt/2))/batch_size)\n",
        "                \n",
        "                # Save threshold when FAR = FRR (=EER)\n",
        "                if diff> abs(FAR-FRR):\n",
        "                    diff = abs(FAR-FRR)\n",
        "                    EER = (FAR+FRR)/2\n",
        "                    EER_thresh = thres\n",
        "                    EER_FAR = FAR\n",
        "                    EER_FRR = FRR\n",
        "            batch_avg_EER += EER\n",
        "            print(\"\\nEER : %0.2f (thres:%0.2f, FAR:%0.2f, FRR:%0.2f)\"%(EER,EER_thresh,EER_FAR,EER_FRR))\n",
        "        avg_EER += batch_avg_EER/(batch_id+1)\n",
        "    print(\"\\n EER across {0} epochs: {1:.4f}\".format(1, avg_EER))\n"
      ],
      "metadata": {
        "id": "0qn3OR6mK6il"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFB-cSWDL7_L",
        "outputId": "7fcb359a-e854-4a65-93fb-8a91ffacffab"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch - 0started\n",
            "Sat May  7 08:34:15 2022\tEpoch:1[30/141],Iteration:30\tLoss:26.4649\tTLoss:26.3836\t\n",
            "\n",
            "Sat May  7 08:34:26 2022\tEpoch:1[60/141],Iteration:60\tLoss:14.6437\tTLoss:24.6774\t\n",
            "\n",
            "Sat May  7 08:34:37 2022\tEpoch:1[90/141],Iteration:90\tLoss:13.0071\tTLoss:23.0980\t\n",
            "\n",
            "Sat May  7 08:34:47 2022\tEpoch:1[120/141],Iteration:120\tLoss:20.6213\tTLoss:22.3128\t\n",
            "\n",
            "epoch - 1started\n",
            "Sat May  7 08:35:06 2022\tEpoch:2[30/141],Iteration:171\tLoss:19.0303\tTLoss:19.2272\t\n",
            "\n",
            "Sat May  7 08:35:16 2022\tEpoch:2[60/141],Iteration:201\tLoss:15.8807\tTLoss:19.5838\t\n",
            "\n",
            "Sat May  7 08:35:27 2022\tEpoch:2[90/141],Iteration:231\tLoss:21.3074\tTLoss:19.1919\t\n",
            "\n",
            "Sat May  7 08:35:38 2022\tEpoch:2[120/141],Iteration:261\tLoss:11.0907\tTLoss:18.9132\t\n",
            "\n",
            "epoch - 2started\n",
            "Sat May  7 08:35:56 2022\tEpoch:3[30/141],Iteration:312\tLoss:14.8836\tTLoss:17.3539\t\n",
            "\n",
            "Sat May  7 08:36:07 2022\tEpoch:3[60/141],Iteration:342\tLoss:20.1234\tTLoss:18.4257\t\n",
            "\n",
            "Sat May  7 08:36:18 2022\tEpoch:3[90/141],Iteration:372\tLoss:12.0562\tTLoss:18.6345\t\n",
            "\n",
            "Sat May  7 08:36:28 2022\tEpoch:3[120/141],Iteration:402\tLoss:24.9226\tTLoss:18.6050\t\n",
            "\n",
            "epoch - 3started\n",
            "Sat May  7 08:36:46 2022\tEpoch:4[30/141],Iteration:453\tLoss:13.6412\tTLoss:15.9432\t\n",
            "\n",
            "Sat May  7 08:36:57 2022\tEpoch:4[60/141],Iteration:483\tLoss:13.7603\tTLoss:16.5246\t\n",
            "\n",
            "Sat May  7 08:37:08 2022\tEpoch:4[90/141],Iteration:513\tLoss:21.4046\tTLoss:16.5033\t\n",
            "\n",
            "Sat May  7 08:37:18 2022\tEpoch:4[120/141],Iteration:543\tLoss:18.6228\tTLoss:16.8273\t\n",
            "\n",
            "epoch - 4started\n",
            "Sat May  7 08:37:37 2022\tEpoch:5[30/141],Iteration:594\tLoss:22.4056\tTLoss:17.0249\t\n",
            "\n",
            "Sat May  7 08:37:47 2022\tEpoch:5[60/141],Iteration:624\tLoss:17.6017\tTLoss:16.7667\t\n",
            "\n",
            "Sat May  7 08:37:58 2022\tEpoch:5[90/141],Iteration:654\tLoss:16.1030\tTLoss:16.2068\t\n",
            "\n",
            "Sat May  7 08:38:09 2022\tEpoch:5[120/141],Iteration:684\tLoss:18.1323\tTLoss:15.7962\t\n",
            "\n",
            "epoch - 5started\n",
            "Sat May  7 08:38:27 2022\tEpoch:6[30/141],Iteration:735\tLoss:19.9881\tTLoss:17.4913\t\n",
            "\n",
            "Sat May  7 08:38:38 2022\tEpoch:6[60/141],Iteration:765\tLoss:21.8245\tTLoss:16.2819\t\n",
            "\n",
            "Sat May  7 08:38:48 2022\tEpoch:6[90/141],Iteration:795\tLoss:8.3410\tTLoss:15.5492\t\n",
            "\n",
            "Sat May  7 08:38:59 2022\tEpoch:6[120/141],Iteration:825\tLoss:16.5064\tTLoss:15.1513\t\n",
            "\n",
            "epoch - 6started\n",
            "Sat May  7 08:39:17 2022\tEpoch:7[30/141],Iteration:876\tLoss:6.4049\tTLoss:13.2830\t\n",
            "\n",
            "Sat May  7 08:39:27 2022\tEpoch:7[60/141],Iteration:906\tLoss:7.8321\tTLoss:13.8356\t\n",
            "\n",
            "Sat May  7 08:39:38 2022\tEpoch:7[90/141],Iteration:936\tLoss:9.4929\tTLoss:14.0826\t\n",
            "\n",
            "Sat May  7 08:39:49 2022\tEpoch:7[120/141],Iteration:966\tLoss:4.9613\tTLoss:13.9884\t\n",
            "\n",
            "epoch - 7started\n",
            "Sat May  7 08:40:07 2022\tEpoch:8[30/141],Iteration:1017\tLoss:18.4690\tTLoss:12.7845\t\n",
            "\n",
            "Sat May  7 08:40:17 2022\tEpoch:8[60/141],Iteration:1047\tLoss:19.4163\tTLoss:12.4365\t\n",
            "\n",
            "Sat May  7 08:40:28 2022\tEpoch:8[90/141],Iteration:1077\tLoss:12.2481\tTLoss:12.3144\t\n",
            "\n",
            "Sat May  7 08:40:39 2022\tEpoch:8[120/141],Iteration:1107\tLoss:16.2942\tTLoss:12.6451\t\n",
            "\n",
            "epoch - 8started\n",
            "Sat May  7 08:40:57 2022\tEpoch:9[30/141],Iteration:1158\tLoss:17.4908\tTLoss:11.5822\t\n",
            "\n",
            "Sat May  7 08:41:07 2022\tEpoch:9[60/141],Iteration:1188\tLoss:10.1252\tTLoss:12.0907\t\n",
            "\n",
            "Sat May  7 08:41:18 2022\tEpoch:9[90/141],Iteration:1218\tLoss:10.6267\tTLoss:12.5081\t\n",
            "\n",
            "Sat May  7 08:41:29 2022\tEpoch:9[120/141],Iteration:1248\tLoss:3.9433\tTLoss:12.5831\t\n",
            "\n",
            "epoch - 9started\n",
            "Sat May  7 08:41:47 2022\tEpoch:10[30/141],Iteration:1299\tLoss:10.3381\tTLoss:11.2874\t\n",
            "\n",
            "Sat May  7 08:41:57 2022\tEpoch:10[60/141],Iteration:1329\tLoss:13.0794\tTLoss:11.2761\t\n",
            "\n",
            "Sat May  7 08:42:08 2022\tEpoch:10[90/141],Iteration:1359\tLoss:0.6135\tTLoss:10.7017\t\n",
            "\n",
            "Sat May  7 08:42:19 2022\tEpoch:10[120/141],Iteration:1389\tLoss:20.9535\tTLoss:10.6537\t\n",
            "\n",
            "\n",
            "Done, trained model saved at speech_id_checkpoint/final_epoch_10_batch_id_141.model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test('./speech_id_checkpoint/final_epoch_10_batch_id_141.model')"
      ],
      "metadata": {
        "id": "URP5rA4sMRXs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "bfe82513-8c7e-41e2-bb61-81966e5d79c7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-9e9d24924c6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./speech_id_checkpoint/final_epoch_10_batch_id_141.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-78afe9ca7022>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mutt\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel_db_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel_db_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0menrollment_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverification_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel_db_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel_db_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0menrollment_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menrollment_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mutt\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menrollment_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menrollment_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    }
  ]
}